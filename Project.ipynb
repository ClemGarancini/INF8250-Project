{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"ad6ec351f9c0496499fdb0e409dcd076","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["# Imports"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import numpy as np\n","import random\n","import pandas as pd\n","from abc import abstractmethod\n","from typing import Tuple\n","import yfinance as yf\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["# Data & Utilities"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[*********************100%%**********************]  1 of 1 completed"]},{"name":"stdout","output_type":"stream","text":["\n","Data for PEP saved to pepsi_data.csv\n","[*********************100%%**********************]  1 of 1 completed\n","Data for KO saved to cola_data.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/bb/28t4vmy165xd8xnxpr121xdc0000gn/T/ipykernel_52739/1131782161.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method=\"ffill\", inplace=True)\n","/var/folders/bb/28t4vmy165xd8xnxpr121xdc0000gn/T/ipykernel_52739/1131782161.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method=\"bfill\", inplace=True)\n","/var/folders/bb/28t4vmy165xd8xnxpr121xdc0000gn/T/ipykernel_52739/1131782161.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method=\"ffill\", inplace=True)\n","/var/folders/bb/28t4vmy165xd8xnxpr121xdc0000gn/T/ipykernel_52739/1131782161.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method=\"bfill\", inplace=True)\n"]}],"source":["def fetch_and_save_stock_data(stock_symbol: str, start_date, end_date, file_name):\n","    stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n","    stock_data = preprocess_data(stock_data)\n","    stock_data.to_csv(file_name)\n","    print(f\"Data for {stock_symbol} saved to {file_name}\")\n","\n","def preprocess_data(df: pd.DataFrame):\n","    df.replace(0, np.nan, inplace=True)\n","\n","    # Forward and backward fill to handle NaNs\n","    df.fillna(method=\"ffill\", inplace=True)\n","    df.fillna(method=\"bfill\", inplace=True)\n","\n","    return df\n","\n","# Example usage\n","fetch_and_save_stock_data(\"PEP\", \"2010-01-01\", \"2023-01-01\", \"pepsi_data.csv\")\n","fetch_and_save_stock_data(\"KO\", \"2010-01-01\", \"2023-01-01\", \"cola_data.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# Basic Environment Class #\n","This class implements the core methods and properties of a trading environment. The different environments that will be needed to implements the experiments will inherit from this class"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class TradingEnvironment:\n","    def __init__(\n","        self, pepsi_file: str, cola_file: str, observation_dim: int, action_dim: int\n","    ):\n","        self.pepsi_data = pd.read_csv(pepsi_file)\n","        self.cola_data = pd.read_csv(cola_file)\n","\n","        self.action_space = range(action_dim)\n","        self.state = np.zeros(observation_dim)\n","\n","        self.current_step = 0\n","        self.portfolio_value = self._compute_portfolio_value()\n","\n","    def step(self, action: int) -> Tuple[np.ndarray, float, bool]:\n","        \"\"\"\n","        Update the environment with action taken by the agent\n","\n","        Args:\n","            action: int, The action taken by the agent\n","\n","        Returns:\n","            next_state_index: int, The index of the next state\n","            reward: float, The reward returned by the environment\n","            done: bool, Is the episode terminated or truncated\n","        \"\"\"\n","        self._check_action_validity(action)\n","        self.state = self._trade(action)\n","        self.current_step += 1\n","        done = self.current_step >= len(self.pepsi_data) - 1\n","        reward = self._compute_reward()\n","\n","        return self.state, reward, done\n","\n","    @abstractmethod\n","    def reset(self) -> np.ndarray:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def _trade(self, action: int) -> np.ndarray:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def _get_indicator(self, stock_data: pd.DataFrame) -> int | float:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def _check_action_validity(self, action: int) -> None:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def _compute_portfolio_value(self) -> float:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    def _get_stock_price(self, step: int, stock_data: pd.DataFrame) -> float:\n","        \"\"\"\n","        Fetch the price for the given step and stock\n","        \"\"\"\n","        return stock_data.iloc[step][\"Close\"]\n","\n","    def _get_stock_trend(self, step: int, stock_data: pd.DataFrame) -> float:\n","        \"\"\"\n","        Fetch the trend for the given stock between the given step and the previous one\n","        \"\"\"\n","        return stock_data.iloc[step][\"Close\"] - stock_data.iloc[step - 1][\"Close\"]\n","\n","    def _compute_reward(self) -> float:\n","        \"\"\"\n","        Computes and updates the portfolio value and returns the reward associated\n","        The reward is the difference between the current portfolio value and the previous one\n","        \"\"\"\n","        current_portfolio_value = self._calculate_portfolio_value()\n","        reward = current_portfolio_value - self.previous_portfolio_value\n","        self.previous_portfolio_value = current_portfolio_value\n","        return reward"]},{"cell_type":"markdown","metadata":{"cell_id":"d12f614c326a433db3380d10cec6d16e","deepnote_cell_type":"code"},"source":["# Experiment 1\n","## Super simplified stock trading as a discrete MDP"]},{"cell_type":"markdown","metadata":{},"source":["### Q Learning\n","To fill"]},{"cell_type":"markdown","metadata":{},"source":["### Environment"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class SimplifiedDiscreteTradingEnvironment(TradingEnvironment):\n","    def __init__(self, pepsi_file: str, cola_file: str):\n","        self.observation_dim = (\n","            5  # [Balance, Shares Pepsi, Shares Cola, Trend Pepsi, Trend Cola]\n","        )\n","        self.action_dim = 4  # 0 = Sell all, 1 = Hold, 2 = Buy Pepsi, 3 = Buy Cola\n","        super.__init__(pepsi_file, cola_file, self.observation_dim, self.action)\n","\n","        self.balance_unit = 10\n","        self.max_balance_units = 10\n","        self.max_shares_per_stock = 5\n","\n","        self.max_state_index = (\n","            11 * 6 * 6 * 2 * 2\n","        )  # 11 balances, 6 shares each for Pepsi and Cola, 2 trends each\n","\n","        self.state = np.array(\n","            [15, 0, 0, 0, 0]\n","        )  # Initial state: [Balance, Pepsi shares, Cola shares, Trend of Pepsi, Trend of Cola]\n","\n","    def __str__(self) -> str:\n","        info = \"\"\"The environment is a Simplified Discrete Trading Problem (Experiment 1).\\n \n","        It is using the stocks: {}, {}.\\n \n","        The episode is at the timestep {}\\n\n","        The current stock prices are {}\\n\n","        Amount of shares held by the agent: {}\\n\n","        Left balance: {}\"\"\"\n","        return info\n","\n","    def step(self, action: int) -> Tuple[np.ndarray, float, bool]:\n","        state, reward, done = super.step(action)\n","        state_index = self.convert_state_to_index(state)\n","        return state_index, reward, done\n","\n","    def reset(self) -> np.ndarray:\n","        self.state = np.array([15, 0, 0, 0, 0])  # Reset to initial state\n","        self.current_step = 0\n","        self.portfolio_value = self._compute_portfolio_value()\n","        return self.state\n","\n","    def _trade(self, action: int) -> np.ndarray:\n","        \"\"\"\n","        Trade the desired amount\n","\n","        Args:\n","            action: int, The trade order, can be\n","                - 0: Sell all\n","                - 1: Hold\n","                - 2: Buy Pepsi\n","                - 3: Buy Cola\n","        \"\"\"\n","        balance_units, shares_pepsi, shares_cola = (\n","            self.state[0],\n","            self.state[1],\n","            self.state[2],\n","        )\n","        balance = balance_units * self.balance_unit\n","        pepsi_price = self._get_stock_price(self.current_step, self.pepsi_data)\n","        cola_price = self._get_stock_price(self.current_step, self.cola_data)\n","\n","        if action == 0:  # Sell all\n","            balance += shares_pepsi * pepsi_price + shares_cola * cola_price\n","            shares_pepsi, shares_cola = 0, 0\n","        elif action == 2:  # Buy Pepsi\n","            quantity = min(\n","                balance // pepsi_price, self.max_shares_per_stock - shares_pepsi\n","            )\n","            shares_pepsi += quantity\n","            balance -= quantity * pepsi_price\n","        elif action == 3:  # Buy Cola\n","            quantity = min(\n","                balance // cola_price, self.max_shares_per_stock - shares_cola\n","            )\n","            shares_cola += quantity\n","            balance -= quantity * cola_price\n","\n","        # Update state with rounded balance\n","        new_balance = max(int(balance / self.balance_unit), 0), self.max_balance_units\n","\n","        trend_pepsi = self._get_indicator(self.current_step, self.pepsi_data)\n","        trend_cola = self._get_indicator(self.current_step, self.cola_data)\n","\n","        return np.array(new_balance, shares_pepsi, shares_cola, trend_pepsi, trend_cola)\n","\n","    def _get_indicator(self, step: int, stock_data: pd.DataFrame) -> int:\n","        trend = self._get_stock_trend(step, stock_data)\n","        return int(trend > 0)\n","\n","    def _compute_portfolio_value(self) -> float:\n","        balance = self.state[0] * self.balance_unit\n","        pepsi_holdings_value = self.state[1] * self._get_stock_price(\n","            self.current_step, self.pepsi_data\n","        )\n","        cola_holdings_value = self.state[2] * self._get_stock_price(\n","            self.current_step, self.cola_data\n","        )\n","        return balance + pepsi_holdings_value + cola_holdings_value\n","\n","    def convert_state_to_index(self, state: np.ndarray) -> int:\n","        balance_index, pepsi_shares, cola_shares, trend_pepsi, trend_cola = state\n","        index = balance_index\n","        index += pepsi_shares * 11\n","        index += cola_shares * 11 * 6\n","        index += trend_pepsi * 11 * 6 * 6\n","        index += trend_cola * 11 * 6 * 6 * 2\n","        return int(index)\n","\n","    def convert_index_to_state(self, index: int) -> np.ndarray:\n","        trend_cola = index // (11 * 6 * 6 * 2)\n","        index %= 11 * 6 * 6 * 2\n","        trend_pepsi = index // (11 * 6 * 6)\n","        index %= 11 * 6 * 6\n","        cola_shares = index // (11 * 6)\n","        index %= 11 * 6\n","        pepsi_shares = index // 11\n","        balance_index = index % 11\n","        return np.array(\n","            [balance_index, pepsi_shares, cola_shares, trend_pepsi, trend_cola]\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["### Agent"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class QLearningAgent:\n","    def __init__(\n","        self,\n","        state_space: int,\n","        action_space: int,\n","        learning_rate=0.01,\n","        discount_factor=0.99,\n","        exploration_rate=1.0,\n","    ):\n","        # Env\n","        self.state_space = state_space\n","        self.action_space = action_space\n","\n","        # Learning\n","        self.learning_rate = learning_rate\n","        self.discount_factor = discount_factor\n","        self.exploration_rate = exploration_rate\n","        self.exploration_min = 0.01\n","        self.exploration_decay = 0.995\n","        self.q_table = np.zeros((state_space, action_space))\n","\n","        # Monitoring\n","        self.q_table_history = np.zeros((1, state_space, action_space))\n","\n","    def __str__(self) -> str:\n","        info = \"\"\"The agent is using Q-Learning algorithm\\n\n","        It is working on Simplified Discrete Trading Environment (Experiment 1)\\n\n","        The current Q Table values can be fetch by calling get_current_q_values() method\\n\n","        The history of Q Table values can be fetch by calling get_history_q_values() method\"\"\"\n","        return info\n","\n","    def choose_action(self, state_index: int) -> int:\n","        \"\"\"\n","        Choose action according to current Q Table\n","        \"\"\"\n","        if np.random.rand() < self.exploration_rate:\n","            return random.randrange(self.action_space)\n","        return np.argmax(self.q_table[state_index])\n","\n","    def train(\n","        self, state_index: int, action: int, reward: float, next_state_index: int\n","    ) -> None:\n","        \"\"\"\n","        Update Q values following Q Learning classical update\n","        \"\"\"\n","        assert 0 <= state_index < self.state_space, \"Invalid state_index\"\n","        assert 0 <= next_state_index < self.state_space, \"Invalid next_state_index\"\n","\n","        q_value = self.q_table[state_index, action]\n","\n","        # Target = Rt + Gamma x max(Q[S(t+1), a])\n","        target = reward + self.discount_factor * np.max(self.q_table[next_state_index])\n","\n","        # Q[S(t), action] =  Q[S(t), action] + alpha x (Rt + Gamma x max(Q[S(t+1), a]) - Q[S(t), action])\n","        self.q_table[state_index, action] += self.learning_rate * (target - q_value)\n","\n","        # Store Q table\n","        self.q_table_history = np.concatenate((self.q_table_history, [self.q_table]))\n","\n","        self.exploration_rate = max(\n","            self.exploration_rate * self.exploration_decay, self.exploration_min\n","        )\n","\n","    def get_current_q_values(self) -> np.ndarray:\n","        \"\"\"\n","        Fetch the current Q Table as a numpy array of shape:\n","            (number of possible states, number of possible actions)\n","        \"\"\"\n","        return self.q_table\n","\n","    def get_history_q_values(self) -> np.ndarray:\n","        \"\"\"\n","        Fetch the history of Q Tables as a numpy array of shape:\n","            (number of episodes seen, number of possible states, number of possible actions)\n","        \"\"\"\n","        return self.q_table_history\n"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def train_QLearning_agent(env: SimplifiedDiscreteTradingEnvironment, \n","                          agent: QLearningAgent, \n","                          num_episodes: int):\n","    \"\"\"\n","    Performs the training of the Agent for experiment 1\n","    \"\"\"\n","\n","    rewards_per_episode = []\n","\n","    for episode in range(num_episodes):\n","        state_index = env.reset()  # Get the initial state index\n","        total_rewards = 0\n","\n","        done = False\n","        while not done:\n","            action = agent.choose_action(state_index)\n","            next_state_index, reward, done = env.step(action)  # next_state_index is directly obtained here\n","            agent.train(state_index, action, reward, next_state_index)\n","            state_index = next_state_index\n","            total_rewards += reward\n","\n","        rewards_per_episode.append(total_rewards)\n","        print(f\"Episode: {episode}, Total Reward: {total_rewards}\")\n","\n","    plt.plot(rewards_per_episode)\n","    plt.title('Rewards per Episode')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Total Reward')\n","    plt.show()\n","\n","    return rewards_per_episode"]},{"cell_type":"markdown","metadata":{},"source":["### Experiment"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# TODO"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_full_width":false,"deepnote_notebook_id":"53e4948ef88741e196477b3b6e01fd0d","kernelspec":{"display_name":"INF8250-Project","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
