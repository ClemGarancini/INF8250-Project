{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"ad6ec351f9c0496499fdb0e409dcd076","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np\n","import random\n","import pandas as pd\n","from abc import abstractmethod\n","from typing import Tuple\n","import yfinance as yf\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"markdown","metadata":{},"source":["# Data & Utilities"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[*********************100%%**********************]  1 of 1 completed\n","Data for PEP saved to pepsi_data.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/bb/28t4vmy165xd8xnxpr121xdc0000gn/T/ipykernel_62693/2278559287.py:19: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method=\"ffill\", inplace=True)\n","/var/folders/bb/28t4vmy165xd8xnxpr121xdc0000gn/T/ipykernel_62693/2278559287.py:20: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method=\"bfill\", inplace=True)\n"]},{"name":"stdout","output_type":"stream","text":["[*********************100%%**********************]  1 of 1 completed\n","Data for KO saved to cola_data.csv\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/bb/28t4vmy165xd8xnxpr121xdc0000gn/T/ipykernel_62693/2278559287.py:19: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method=\"ffill\", inplace=True)\n","/var/folders/bb/28t4vmy165xd8xnxpr121xdc0000gn/T/ipykernel_62693/2278559287.py:20: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df.fillna(method=\"bfill\", inplace=True)\n"]}],"source":["def fetch_and_save_stock_data(stock_symbol: str, start_date, end_date, file_name):\n","    \"\"\"\n","    Fetch data from yf database and store it in the current repository.\n","    Once it is done, the data can be accessed using pd.read_csv(filename)\n","    \"\"\"\n","    stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n","    stock_data = preprocess_data(stock_data)\n","    stock_data.to_csv(file_name)\n","    print(f\"Data for {stock_symbol} saved to {file_name}\")\n","\n","\n","def preprocess_data(df: pd.DataFrame):\n","    \"\"\"\n","    Preprocess the data for our experiments\n","    \"\"\"\n","    df.replace(0, np.nan, inplace=True)\n","\n","    # Forward and backward fill to handle NaNs\n","    df.fillna(method=\"ffill\", inplace=True)\n","    df.fillna(method=\"bfill\", inplace=True)\n","\n","    return df\n","\n","\n","# Example usage\n","fetch_and_save_stock_data(\"PEP\", \"2010-01-01\", \"2023-01-01\", \"pepsi_data.csv\")\n","fetch_and_save_stock_data(\"KO\", \"2010-01-01\", \"2023-01-01\", \"cola_data.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# Basic Environment Class #\n","This class implements the core methods and properties of a trading environment. The different environments that will be needed to implements the experiments will inherit from this class"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class TradingEnvironment:\n","    def __init__(\n","        self, pepsi_file: str, cola_file: str, observation_dim: int, action_dim: int\n","    ):\n","        self.observation_dim = observation_dim\n","        self.action_dim = action_dim\n","        self.pepsi_data = pd.read_csv(pepsi_file)\n","        self.pepsi_data.Name = \"Pepsi\"\n","        self.cola_data = pd.read_csv(cola_file)\n","        self.cola_data.Name = \"Cola\"\n","\n","        self.action_space = range(action_dim)\n","        self.state = np.zeros(observation_dim)\n","\n","        self.current_step = 0\n","        self.portfolio_value = self._compute_portfolio_value()\n","\n","    def step(self, action: int) -> Tuple[np.ndarray, float, bool]:\n","        \"\"\"\n","        Update the environment with action taken by the agent\n","\n","        Args:\n","            action: int, The action taken by the agent\n","\n","        Returns:\n","            next_state_index: int, The index of the next state\n","            reward: float, The reward returned by the environment\n","            done: bool, Is the episode terminated or truncated\n","        \"\"\"\n","        self._check_action_validity(action)\n","        self.state = self._trade(action)\n","        self.current_step += 1\n","        done = self.current_step >= len(self.pepsi_data) - 1\n","        reward = self._compute_reward()\n","\n","        return self.state, reward, done\n","\n","    @abstractmethod\n","    def reset(self) -> np.ndarray:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def _trade(self, action: int) -> np.ndarray:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def _get_indicator(self, stock_data: pd.DataFrame) -> int | float:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def _check_action_validity(self, action: int) -> None:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def _compute_portfolio_value(self) -> float:\n","        # Not Implemented\n","        raise NotImplementedError\n","\n","    def _get_stock_price(self, step: int, stock_data: pd.DataFrame) -> float:\n","        \"\"\"\n","        Fetch the price for the given step and stock\n","        \"\"\"\n","        return stock_data.iloc[step][\"Close\"]\n","\n","    def _get_stock_trend(self, step: int, stock_data: pd.DataFrame) -> float:\n","        \"\"\"\n","        Fetch the trend for the given stock between the given step and the previous one\n","        \"\"\"\n","        return stock_data.iloc[step][\"Close\"] - stock_data.iloc[step - 1][\"Close\"]\n","\n","    def _compute_reward(self) -> float:\n","        \"\"\"\n","        Computes and updates the portfolio value and returns the reward associated\n","        The reward is the difference between the current portfolio value and the previous one\n","        \"\"\"\n","        current_portfolio_value = self._compute_portfolio_value()\n","        reward = current_portfolio_value - self.portfolio_value\n","        self.portfolio_value = current_portfolio_value\n","        return reward"]},{"cell_type":"markdown","metadata":{"cell_id":"d12f614c326a433db3380d10cec6d16e","deepnote_cell_type":"code"},"source":["# Experiment 1\n","## Super simplified stock trading as a discrete MDP"]},{"cell_type":"markdown","metadata":{},"source":["### Q Learning\n","To fill"]},{"cell_type":"markdown","metadata":{},"source":["### Environment"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class SimplifiedDiscreteTradingEnvironment(TradingEnvironment):\n","    def __init__(self, pepsi_file: str, cola_file: str):\n","        self.observation_dim = (\n","            5  # [Balance, Shares Pepsi, Shares Cola, Trend Pepsi, Trend Cola]\n","        )\n","        self.action_dim = 4  # 0 = Sell all, 1 = Hold, 2 = Buy Pepsi, 3 = Buy Cola\n","\n","        self.balance_unit = 10\n","        self.max_balance_units = 10\n","        self.max_shares_per_stock = 5\n","\n","        self.max_state_index = (\n","            11 * 6 * 6 * 2 * 2\n","        )  # 11 balances, 6 shares each for Pepsi and Cola, 2 trends each\n","\n","        super().__init__(pepsi_file, cola_file, self.observation_dim, self.action_dim)\n","\n","        self.state = np.array(\n","            [15, 0, 0, 0, 0]\n","        )  # Initial state: [Balance, Pepsi shares, Cola shares, Trend of Pepsi, Trend of Cola]\n","\n","    def __str__(self) -> str:\n","        info = \"\"\"The environment is a Simplified Discrete Trading Problem (Experiment 1).\\n \n","        It is using the stocks: {}, {}\n","        The episode is at the timestep {}\n","        The current stock prices are {}$ and {}$\n","        Amount of shares held by the agent: {}\n","        Left balance: {}\"\"\".format(\n","            self.pepsi_data.Name,\n","            self.cola_data.Name,\n","            self.current_step,\n","            np.round(self._get_stock_price(self.current_step, self.pepsi_data), 2),\n","            np.round(self._get_stock_price(self.current_step, self.cola_data), 2),\n","            self.state[1:3],\n","            self.state[0],\n","        )\n","\n","        return info\n","\n","    def step(self, action: int) -> Tuple[np.ndarray, float, bool]:\n","        state, reward, done = super().step(action)\n","        state_index = self.convert_state_to_index(state)\n","        return state_index, reward, done\n","\n","    def reset(self) -> np.ndarray:\n","        self.state = np.array([15, 0, 0, 0, 0])  # Reset to initial state\n","        self.current_step = 0\n","        self.portfolio_value = self._compute_portfolio_value()\n","        return self.state\n","\n","    def _trade(self, action: int) -> np.ndarray:\n","        \"\"\"\n","        Trade the desired amount\n","\n","        Args:\n","            action: int, The trade order, can be\n","                - 0: Sell all\n","                - 1: Hold\n","                - 2: Buy Pepsi\n","                - 3: Buy Cola\n","        \"\"\"\n","        balance_units, shares_pepsi, shares_cola = (\n","            self.state[0],\n","            self.state[1],\n","            self.state[2],\n","        )\n","        balance = balance_units * self.balance_unit\n","        pepsi_price = self._get_stock_price(self.current_step, self.pepsi_data)\n","        cola_price = self._get_stock_price(self.current_step, self.cola_data)\n","\n","        if action == 0:  # Sell all\n","            balance += shares_pepsi * pepsi_price + shares_cola * cola_price\n","            shares_pepsi, shares_cola = 0, 0\n","        elif action == 2:  # Buy Pepsi\n","            quantity = min(\n","                balance // pepsi_price, self.max_shares_per_stock - shares_pepsi\n","            )\n","            shares_pepsi += quantity\n","            balance -= quantity * pepsi_price\n","        elif action == 3:  # Buy Cola\n","            quantity = min(\n","                balance // cola_price, self.max_shares_per_stock - shares_cola\n","            )\n","            shares_cola += quantity\n","            balance -= quantity * cola_price\n","\n","        # Update state with rounded balance\n","        new_balance = min(int(balance / self.balance_unit), self.max_balance_units)\n","\n","        trend_pepsi = self._get_indicator(self.current_step, self.pepsi_data)\n","        trend_cola = self._get_indicator(self.current_step, self.cola_data)\n","\n","        return np.array(\n","            [new_balance, shares_pepsi, shares_cola, trend_pepsi, trend_cola]\n","        )\n","\n","    def _get_indicator(self, step: int, stock_data: pd.DataFrame) -> int:\n","        trend = self._get_stock_trend(step, stock_data)\n","        return int(trend > 0)\n","\n","    def _check_action_validity(self, action: int) -> None:\n","        if action not in range(self.action_dim):\n","            raise ValueError(\"Action must be in [0,{}]\".format(self.action_dim - 1))\n","\n","    def _compute_portfolio_value(self) -> float:\n","        balance = self.state[0] * self.balance_unit\n","        pepsi_holdings_value = self.state[1] * self._get_stock_price(\n","            self.current_step, self.pepsi_data\n","        )\n","        cola_holdings_value = self.state[2] * self._get_stock_price(\n","            self.current_step, self.cola_data\n","        )\n","        return balance + pepsi_holdings_value + cola_holdings_value\n","\n","    def convert_state_to_index(self, state: np.ndarray) -> int:\n","        balance_index, pepsi_shares, cola_shares, trend_pepsi, trend_cola = state\n","        index = balance_index\n","        index += pepsi_shares * 11\n","        index += cola_shares * 11 * 6\n","        index += trend_pepsi * 11 * 6 * 6\n","        index += trend_cola * 11 * 6 * 6 * 2\n","        return int(index)\n","\n","    def convert_index_to_state(self, index: int) -> np.ndarray:\n","        trend_cola = index // (11 * 6 * 6 * 2)\n","        index %= 11 * 6 * 6 * 2\n","        trend_pepsi = index // (11 * 6 * 6)\n","        index %= 11 * 6 * 6\n","        cola_shares = index // (11 * 6)\n","        index %= 11 * 6\n","        pepsi_shares = index // 11\n","        balance_index = index % 11\n","        return np.array(\n","            [balance_index, pepsi_shares, cola_shares, trend_pepsi, trend_cola]\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["### Agent"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class QLearningAgent:\n","    def __init__(\n","        self,\n","        state_space: int,\n","        action_space: int,\n","        learning_rate=0.01,\n","        discount_factor=0.99,\n","        exploration_rate=1.0,\n","    ):\n","        # Env\n","        self.state_space = state_space\n","        self.max_state_index = 11 * 6 * 6 * 2 * 2\n","        self.action_space = action_space\n","\n","        # Learning\n","        self.learning_rate = learning_rate\n","        self.discount_factor = discount_factor\n","        self.exploration_rate = exploration_rate\n","        self.exploration_min = 0.01\n","        self.exploration_decay = 0.995\n","        self.q_table = np.zeros((self.max_state_index, action_space))\n","\n","        # Monitoring\n","        self.q_table_history = np.zeros((1, self.max_state_index, action_space))\n","\n","    def __str__(self) -> str:\n","        info = \"\"\"The agent is using Q-Learning algorithm\\n\n","        It is working on Simplified Discrete Trading Environment (Experiment 1)\\n\n","        The current Q Table values can be fetch by calling get_current_q_values() method\\n\n","        The history of Q Table values can be fetch by calling get_history_q_values() method\"\"\"\n","        return info\n","\n","    def choose_action(self, state_index: int) -> int:\n","        \"\"\"\n","        Choose action according to current Q Table\n","        \"\"\"\n","        if np.random.rand() < self.exploration_rate:\n","            return random.randrange(self.action_space)\n","        return np.argmax(self.q_table[state_index])\n","\n","    def train(\n","        self, state_index: int, action: int, reward: float, next_state_index: int\n","    ) -> None:\n","        \"\"\"\n","        Update Q values following Q Learning classical update\n","        \"\"\"\n","        assert 0 <= state_index < self.max_state_index, \"Invalid state_index\"\n","        assert 0 <= next_state_index < self.max_state_index, \"Invalid next_state_index\"\n","\n","        q_value = self.q_table[state_index, action]\n","\n","        # Target = Rt + Gamma x max(Q[S(t+1), a])\n","        target = reward + self.discount_factor * np.max(self.q_table[next_state_index])\n","\n","        # Q[S(t), action] =  Q[S(t), action] + alpha x (Rt + Gamma x max(Q[S(t+1), a]) - Q[S(t), action])\n","        self.q_table[state_index, action] += self.learning_rate * (target - q_value)\n","\n","        # Store Q table\n","        self.q_table_history = np.concatenate((self.q_table_history, [self.q_table]))\n","\n","        self.exploration_rate = max(\n","            self.exploration_rate * self.exploration_decay, self.exploration_min\n","        )\n","\n","    def get_current_q_values(self) -> np.ndarray:\n","        \"\"\"\n","        Fetch the current Q Table as a numpy array of shape:\n","            (number of possible states, number of possible actions)\n","        \"\"\"\n","        return self.q_table\n","\n","    def get_history_q_values(self) -> np.ndarray:\n","        \"\"\"\n","        Fetch the history of Q Tables as a numpy array of shape:\n","            (number of episodes seen, number of possible states, number of possible actions)\n","        \"\"\"\n","        return self.q_table_history"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["def train_QLearning_agent(\n","    env: SimplifiedDiscreteTradingEnvironment, agent: QLearningAgent, num_episodes: int\n","):\n","    \"\"\"\n","    Performs the training of the Agent for experiment 1\n","    \"\"\"\n","\n","    rewards_per_episode = []\n","\n","    for episode in range(num_episodes):\n","        state_index = env.convert_state_to_index(\n","            env.reset()\n","        )  # Get the initial state index\n","        total_rewards = 0\n","\n","        done = False\n","        while not done:\n","            action = agent.choose_action(state_index)\n","            next_state_index, reward, done = env.step(\n","                action\n","            )  # next_state_index is directly obtained here\n","            agent.train(state_index, action, reward, next_state_index)\n","            state_index = next_state_index\n","            total_rewards += reward\n","\n","        rewards_per_episode.append(total_rewards)\n","        print(f\"Episode: {episode}, Total Reward: {total_rewards}\")\n","\n","    plt.plot(rewards_per_episode)\n","    plt.title(\"Rewards per Episode\")\n","    plt.xlabel(\"Episode\")\n","    plt.ylabel(\"Total Reward\")\n","    plt.show()\n","\n","    return rewards_per_episode"]},{"cell_type":"markdown","metadata":{},"source":["### Experiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EnvExp1 = SimplifiedDiscreteTradingEnvironment(\"pepsi_data.csv\", \"cola_data.csv\")\n","AgentExp1 = QLearningAgent(state_space=5, action_space=4)\n","rewards = train_QLearning_agent(env=EnvExp1, agent=AgentExp1, num_episodes=1000)"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment 2\n","## Super simplified stock trading as a continuous MDP"]},{"cell_type":"markdown","metadata":{},"source":["### Environment"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["class SimplifiedContinuousTradingEnvironmentDQN(TradingEnvironment):\n","    def __init__(self, pepsi_file, cola_file):\n","        observation_dim = (\n","            5  # balance, shares of Pepsi, shares of Cola, trend of Pepsi, trend of Cola\n","        )\n","        action_dim = 4  # Sell all, Hold, Buy Pepsi, Buy Cola\n","        super().__init__(\n","            pepsi_file,\n","            cola_file,\n","            observation_dim=observation_dim,\n","            action_dim=action_dim,\n","        )\n","\n","        self.state = np.array([10000.0, 0, 0, 0, 0])  # Initial state\n","\n","    def __str__(self) -> str:\n","        info = \"\"\"The environment is a Simplified Continuous Trading Problem (Experiment 2).\\n \n","        It is using the stocks: {}, {}\n","        The episode is at the timestep {}\n","        The current stock prices are {}$ and {}$\n","        Amount of shares held by the agent: {}\n","        Left balance: {}\"\"\".format(\n","            self.pepsi_data.Name,\n","            self.cola_data.Name,\n","            self.current_step,\n","            np.round(self._get_stock_price(self.current_step, self.pepsi_data), 2),\n","            np.round(self._get_stock_price(self.current_step, self.cola_data), 2),\n","            self.state[1:3],\n","            self.state[0],\n","        )\n","\n","        return info\n","\n","    def step(self, action: int) -> Tuple[np.ndarray, float, bool]:\n","        return super().step(action)\n","\n","    def reset(self) -> np.ndarray:\n","        self.state = np.array([10000.0, 0, 0, 0, 0])  # Reset to initial state\n","        self.current_step = 0\n","        return self.state\n","\n","    def _trade(self, action: int) -> np.ndarray:\n","        \"\"\"\n","        Trade the desired amount\n","\n","        Args:\n","            action: int, The trade order, can be\n","                - 0: Sell all\n","                - 1: Hold\n","                - 2: Buy Pepsi\n","                - 3: Buy Cola\n","        \"\"\"\n","        balance, shares_pepsi, shares_cola = self.state[0], self.state[1], self.state[2]\n","        pepsi_price = self._get_stock_price(self.current_step, self.pepsi_data)\n","        cola_price = self._get_stock_price(self.current_step, self.cola_data)\n","\n","        if action == 0:  # Sell all\n","            balance += shares_pepsi * pepsi_price + shares_cola * cola_price\n","            shares_pepsi, shares_cola = 0, 0\n","        elif action == 2:  # Buy Pepsi\n","            quantity = balance / pepsi_price\n","            if quantity > 0:\n","                shares_pepsi += quantity\n","                balance -= quantity * pepsi_price\n","        elif action == 3:  # Buy Cola\n","            quantity = balance / cola_price\n","            if quantity > 0:\n","                shares_cola += quantity\n","                balance -= quantity * cola_price\n","\n","        trend_pepsi = self._get_indicator(self.current_step, self.pepsi_data)\n","        trend_cola = self._get_indicator(self.current_step, self.cola_data)\n","\n","        return np.array([balance, shares_pepsi, shares_cola, trend_pepsi, trend_cola])\n","\n","    def _get_indicator(self, step: int, stock_data: pd.DataFrame) -> float:\n","        return self._get_stock_trend(step, stock_data)\n","\n","    def _check_action_validity(self, action: int) -> None:\n","        if action not in range(self.action_dim):\n","            raise ValueError(\"Action must be in [0,{}]\".format(self.action_dim - 1))\n","\n","    def _compute_portfolio_value(self) -> float:\n","        # Compute total portfolio value\n","        pepsi_price = self._get_stock_price(self.current_step, self.pepsi_data)\n","        cola_price = self._get_stock_price(self.current_step, self.cola_data)\n","        return self.state[0] + self.state[1] * pepsi_price + self.state[2] * cola_price"]},{"cell_type":"markdown","metadata":{},"source":["### Agent"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = []  # Memory for experience replay\n","\n","        # Hyperparameters\n","        self.gamma = 0.95  # Discount rate\n","        self.epsilon = 1.0  # Exploration rate\n","        self.epsilon_decay = 0.995\n","        self.epsilon_min = 0.01\n","        self.learning_rate = 0.001\n","\n","        self.model = self._build_model()\n","\n","    def _build_model(self):\n","        # Define a simple Neural Network model\n","        model = nn.Sequential(\n","            nn.Linear(self.state_size, 24),\n","            nn.ReLU(),\n","            nn.Linear(24, 24),\n","            nn.ReLU(),\n","            nn.Linear(24, self.action_size),\n","        )\n","        self.optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n","        return model\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def choose_action(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        state = torch.from_numpy(state).float()\n","        with torch.no_grad():\n","            act_values = self.model(state)\n","        return np.argmax(act_values.cpu().data.numpy())\n","\n","    def replay(self, batch_size):\n","        if len(self.memory) < batch_size:\n","            return\n","        minibatch = random.sample(self.memory, batch_size)\n","        for state, action, reward, next_state, done in minibatch:\n","            state = torch.from_numpy(state).float()\n","            next_state = torch.from_numpy(next_state).float()\n","            reward = torch.tensor(reward)\n","            action = torch.tensor(action)\n","            done = torch.tensor(done)\n","\n","            target = reward\n","            if not done:\n","                target = (\n","                    reward\n","                    + self.gamma * torch.max(self.model(next_state).detach()).item()\n","                )\n","            target_f = self.model(state)\n","            target_f[0][action] = target\n","\n","            self.optimizer.zero_grad()\n","            loss = nn.functional.mse_loss(target_f, self.model(state))\n","            loss.backward()\n","            self.optimizer.step()\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def train_dqn_agent(\n","    env: SimplifiedContinuousTradingEnvironmentDQN,\n","    agent: DQNAgent,\n","    num_episodes: int = 1000,\n","    batch_size: int = 32,\n","):\n","    rewards_per_episode = []\n","\n","    for episode in range(num_episodes):\n","        state = env.reset()\n","        state = np.reshape(\n","            state, [1, env.observation_dim]\n","        )  # Reshape for neural network compatibility\n","        total_rewards = 0\n","\n","        done = False\n","        while not done:\n","            action = agent.choose_action(state)\n","            next_state, reward, done = env.step(action)\n","            next_state = np.reshape(next_state, [1, env.observation_dim])\n","\n","            agent.remember(\n","                state, action, reward, next_state, done\n","            )  # Remember the experience\n","            state = next_state\n","            total_rewards += reward\n","\n","        agent.replay(batch_size)  # Train the model with experiences in memory\n","\n","        rewards_per_episode.append(total_rewards)\n","        print(\n","            f\"Episode: {episode + 1}, Reward: {total_rewards}, Epsilon: {agent.epsilon}\"\n","        )\n","\n","        # Optionally implement a check for early stopping or model saving\n","\n","    # Plot the rewards\n","    plt.plot(rewards_per_episode)\n","    plt.title(\"Rewards per Episode\")\n","    plt.xlabel(\"Episode\")\n","    plt.ylabel(\"Total Reward\")\n","    plt.show()\n","\n","    return rewards_per_episode"]},{"cell_type":"markdown","metadata":{},"source":["### Experiment"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["EnvExp2 = SimplifiedContinuousTradingEnvironmentDQN(\"pepsi_data.csv\", \"cola_data.csv\")\n","AgentExp2 = DQNAgent(EnvExp2.observation_dim, EnvExp2.action_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_full_width":false,"deepnote_notebook_id":"53e4948ef88741e196477b3b6e01fd0d","kernelspec":{"display_name":"INF8250-Project","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
